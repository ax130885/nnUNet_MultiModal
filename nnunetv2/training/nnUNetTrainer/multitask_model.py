# nnunetv2/training/nnUNetTrainer/multitask_segmentation_model.py
import os
import torch
import numpy as np
from torch import nn
from torch.nn.modules.instancenorm import InstanceNorm3d
from torch.nn.modules.activation import LeakyReLU
from nnunetv2.preprocessing.clinical_data_label_encoder import ClinicalDataLabelEncoder
from sentence_transformers import SentenceTransformer

import matplotlib.pyplot as plt
import numpy as np


class TextEncoderModule(nn.Module):
    """
    ç¨ç«‹çš„æ–‡å­—ç·¨ç¢¼æ¨¡çµ„ï¼Œä½¿ç”¨ torch._dynamo.disable é¿å…è¢«ç·¨è­¯
    """
    def __init__(self, freeze_bert: bool = True):
        super().__init__()
        self.sentence_transformer = SentenceTransformer("neuml/pubmedbert-base-embeddings")
        
        # å‡çµæ¬Šé‡
        if freeze_bert:
            for param in self.sentence_transformer.parameters():
                param.requires_grad = False
    
    @torch._dynamo.disable  # é€™æ˜¯é—œéµï¼šå‘Šè¨´ torch.compile ä¸è¦ç·¨è­¯é€™å€‹æ–¹æ³•
    def forward(self, text_descriptions):
        """
        å°æ–‡å­—æè¿°é€²è¡Œç·¨ç¢¼ï¼Œæ­¤æ–¹æ³•ä¸æœƒè¢« torch.compile ç·¨è­¯
        """
        embeddings = self.sentence_transformer.encode(
            text_descriptions, 
            convert_to_tensor=True
        )
        return embeddings.detach()


class ASPP3D(nn.Module):
    """
    3D Atrous Spatial Pyramid Pooling (ASPP) æ¨¡å¡Š
    ä½¿ç”¨ä¸åŒçš„è†¨è„¹ç‡ä¾†æ•ç²å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    def __init__(self, in_channels: int, out_channels: int, rates: list = [1, 2, 3, 4]):
        super().__init__()
        
        # 1x1x1 å·ç©åˆ†æ”¯
        self.conv1 = nn.Sequential(
            nn.Conv3d(in_channels, out_channels, 1, bias=False),
            InstanceNorm3d(out_channels),
            nn.LeakyReLU(0.01, inplace=True)
        )
        
        # å¤šå€‹è†¨è„¹å·ç©åˆ†æ”¯
        self.atrous_convs = nn.ModuleList()
        for rate in rates:
            self.atrous_convs.append(
                nn.Sequential(
                    nn.Conv3d(in_channels, out_channels, 3, 
                             padding=rate, dilation=rate, bias=False),
                    InstanceNorm3d(out_channels),
                    nn.LeakyReLU(0.01, inplace=True)
                )
            )
        
        # å…¨å±€å¹³å‡æ± åŒ–åˆ†æ”¯
        # æ³¨æ„ï¼šæ± åŒ–å¾Œæ˜¯ [B, C, 1, 1, 1]ï¼Œä¸èƒ½ä½¿ç”¨ InstanceNorm3dï¼ˆéœ€è¦ >1 ç©ºé–“å…ƒç´ ï¼‰
        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool3d(1),
            nn.Conv3d(in_channels, out_channels, 1, bias=True),  # ä½¿ç”¨ bias è£œå„Ÿæ²’æœ‰ norm
            nn.LeakyReLU(0.01, inplace=True)
        )
        
        # èåˆæ‰€æœ‰åˆ†æ”¯å¾Œçš„æŠ•å½±å±¤
        # ç¸½å…±æœ‰: 1å€‹1x1å·ç© + len(rates)å€‹è†¨è„¹å·ç© + 1å€‹å…¨å±€æ± åŒ– = len(rates)+2 å€‹åˆ†æ”¯
        total_branches = len(rates) + 2
        self.project = nn.Sequential(
            nn.Conv3d(out_channels * total_branches, out_channels, 1, bias=False),
            InstanceNorm3d(out_channels),
            nn.LeakyReLU(0.01, inplace=True),
            nn.Dropout3d(0.1)
        )
        
        # åˆå§‹åŒ–æ¬Šé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, nn.Conv3d):
            nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu', a=0.01)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, InstanceNorm3d):
            if m.weight is not None:
                nn.init.constant_(m.weight, 1)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """
        x: [B, C, D, H, W]
        """
        # ä¿å­˜è¼¸å…¥çš„ç©ºé–“å°ºå¯¸
        size = x.shape[2:]
        
        # 1x1 å·ç©åˆ†æ”¯
        feat1 = self.conv1(x)
        
        # è†¨è„¹å·ç©åˆ†æ”¯
        atrous_feats = [conv(x) for conv in self.atrous_convs]
        
        # å…¨å±€å¹³å‡æ± åŒ–åˆ†æ”¯
        global_feat = self.global_avg_pool(x)
        # ä¸Šæ¡æ¨£åˆ°åŸå§‹å°ºå¯¸
        global_feat = torch.nn.functional.interpolate(
            global_feat, size=size, mode='trilinear', align_corners=False
        )
        
        # æ‹¼æ¥æ‰€æœ‰åˆ†æ”¯
        concat_feats = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)
        
        # æŠ•å½±åˆ°è¼¸å‡ºç¶­åº¦
        out = self.project(concat_feats)
        
        return out


class FiLMAddFusion(nn.Module):
    """
    ç”¨ FiLM æŠŠè‡¨åºŠå‘é‡è®Šæˆ Î³ã€Î²ï¼Œå°å½±åƒ feature åšé€é€šé“ä»¿å°„è®Šæ›ã€‚
    è®Šæ›å¾Œçš„ feature å†èˆ‡ skip é€£æ¥çš„ feature **concat**ï¼ˆä¸æ”¹ nnUNet é‚è¼¯ï¼‰ã€‚
    """
    def __init__(self, channels: int, cli_dim: int):
        super().__init__()
        # å½±åƒæŠ•å½± (ä¿æŒé€šé“æ•¸)
        self.img_proj = nn.Sequential(
            nn.Conv3d(channels, channels, 1),
            nn.LeakyReLU(0.01),
            InstanceNorm3d(channels)
        )

        # è‡¨åºŠ â†’ Î³ã€Î² å…©çµ„é€šé“æ¬Šé‡
        self.film = nn.Sequential(
            nn.Linear(cli_dim, channels * 2),
            nn.LeakyReLU(0.01)
        )

        # å¾Œè™•ç†å·ç©
        self.final_fusion = nn.Sequential(
            nn.Conv3d(channels, channels * 2, 3, padding=1),
            nn.GELU(),
            nn.Conv3d(channels * 2, channels, 3, padding=1),
            InstanceNorm3d(channels)
        )
        
        # ä½¿ç”¨çµ±ä¸€çš„åˆå§‹åŒ–æ–¹æ³•
        self.apply(self._init_weights)

    def _init_weights(self, m):
        """åˆå§‹åŒ–æ¬Šé‡ï¼Œç¬¦åˆ apply æ–¹æ³•çš„åƒæ•¸è¦æ±‚"""
        if isinstance(m, nn.Conv3d):
            nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu', a=0.01)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu', a=0.01)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, InstanceNorm3d):
            if m.weight is not None:
                nn.init.constant_(m.weight, 1)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, img_feat, cli_feat):
        """
        img_feat : [B, C, D, H, W]
        cli_feat : [B, cli_dim]  (å·²ç¶“æ˜¯ 320 æˆ– 128 ç¶­)
        """
        # 1. å½±åƒæŠ•å½±
        x = self.img_proj(img_feat)           # [B, C, D, H, W]

        # 2. FiLM ç”¢ç”Ÿ Î³, Î²
        g_b = self.film(cli_feat)             # [B, 2C]
        gamma, beta = g_b.chunk(2, dim=1)     # æ‹†æˆå…©å€‹ [B, C]
        B, C = gamma.shape
        gamma = gamma.view(B, C, 1, 1, 1)
        beta  = beta.view(B, C, 1, 1, 1)

        # 3. ä»¿å°„è®Šæ›
        x = x * gamma + beta                  # é€é€šé“èª¿è£½

        # 4. å¾Œè™•ç† + æ®‹å·®
        out = x + self.final_fusion(x)
        return out
    


class GatedFusion(nn.Module):
    def __init__(self, channels: int):
        super().__init__()

        # æ”¹é€²1ï¼šéç·šæ€§æŠ•å½±å±¤
        # å½±åƒç‰¹å¾µä½¿ç”¨Conv3Dé€²è¡ŒæŠ•å½±
        self.img_proj = nn.Sequential(
            nn.Conv3d(channels, channels, 1),
            nn.LeakyReLU(0.01),
            InstanceNorm3d(channels)
        )
        # è‡¨åºŠç‰¹å¾µä½¿ç”¨MLPè€ŒéConv3Dï¼Œå› ç‚ºçµæ§‹åŒ–æ•¸æ“šç„¡ç©ºé–“é—œä¿‚
        self.cli_proj = nn.Sequential(
            nn.Linear(320, 128), # å…ˆå°‡320ç¶­åŸå§‹è‡¨åºŠç‰¹å¾µæŠ•å½±åˆ°128ç¶­
            nn.LeakyReLU(0.01),
            nn.Linear(128, channels), # å†å°‡128ç¶­æŠ•å½±åˆ°èˆ‡å½±åƒç‰¹å¾µç›¸åŒçš„é€šé“æ•¸ (é¿å…320ç›´æ¥å£“åˆ°32å·®è·å¤ªå¤§)
            nn.LayerNorm(channels)
        )
        
        # æ”¹é€²2ï¼šé–€æ§æ©Ÿåˆ¶: è¼¸å…¥ æ‹¼æ¥å…©å€‹ç‰¹å¾µçš„projï¼Œè¼¸å‡º å…©å€‹ç‰¹å¾µå°‡ä¹˜ä¸Šçš„æ¬Šé‡å¼µé‡
        # æ··åˆæ–¹å¼åœ¨forwardingå…§å¯¦ç¾ï¼Œç›®å‰æ˜¯åœ¨channelç¶­åº¦é€²è¡Œconcateï¼Œchannelç¶­åº¦æœƒç¿»å€
        self.gate_conv = nn.Sequential(
            nn.Conv3d(channels*2, channels//2, 3, padding=1),
            nn.LeakyReLU(0.01),
            nn.Conv3d(channels//2, channels, 1),
            nn.Sigmoid()
        )

        # æ”¹é€²3ï¼šé–€æ§å¾Œçš„çµæœ å¥—ä¸€å±¤MLPå†è¼¸å‡º
        self.final_fusion = nn.Sequential(
            nn.Conv3d(channels, channels*2, 3, padding=1),
            nn.GELU(), # æ›´è¤‡é›œçš„ activation functionï¼Œå¯ä»¥æ›ç‚º leaky ReLU
            nn.Conv3d(channels*2, channels, 3, padding=1),
            InstanceNorm3d(channels)
        )
        
        # ä½¿ç”¨çµ±ä¸€çš„åˆå§‹åŒ–æ–¹æ³•
        self.apply(self._init_weights)

    def _init_weights(self, m):
        """åˆå§‹åŒ–æ¬Šé‡ï¼Œç¬¦åˆ apply æ–¹æ³•çš„åƒæ•¸è¦æ±‚"""
        if isinstance(m, nn.Conv3d):
            # å°æ–¼é–€æ§æ©Ÿåˆ¶ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–
            if any(m is layer for layer in self.gate_conv):
                # é–€æ§å±¤çš„æ¬Šé‡åˆå§‹åŒ–ç‚ºæ¥è¿‘é›¶çš„å€¼ï¼Œä½¿åˆå§‹é–€æ§æ¥è¿‘0.5
                nn.init.normal_(m.weight, mean=0.0, std=0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            else:
                # å…¶ä»–å·ç©å±¤ä½¿ç”¨Kaimingåˆå§‹åŒ–
                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu', a=0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu', a=0.01)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (InstanceNorm3d, nn.LayerNorm)):
            if m.weight is not None:
                nn.init.constant_(m.weight, 1)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, img_feat, cli_feat):
        # å½±åƒç‰¹å¾µæŠ•å½± (conv)
        proj_img = self.img_proj(img_feat)
        # è‡¨åºŠç‰¹å¾µæŠ•å½± (MLP)
        proj_cli_1d = self.cli_proj(cli_feat)

        # å°‡è‡¨åºŠæŠ•å½±æ”¹ç‚ºç¬¦åˆå½±åƒæŠ•å½±çš„ç¶­åº¦
         # [B, C] -> [B, C, 1, 1, 1]
        proj_cli = proj_cli_1d.view(*proj_cli_1d.shape, 1, 1, 1)
        # [B, C, 1, 1, 1] -> [B, C, D, H, W]
        proj_cli = proj_cli.expand(-1, -1, *img_feat.shape[2:])  # -1ä»£è¡¨ä¸è®Š, [2:] ç›¸ç•¶æ–¼è¼¸å…¥ä¸‰å€‹åƒæ•¸, æŒ‡å®šDHWèˆ‡å½±åƒçš„ç¶­åº¦ç›¸åŒ
        

        # é›™è·¯é–€æ§
        combined = torch.cat([proj_img, proj_cli], dim=1)  # æ²¿é€šé“ç¶­æ‹¼æ¥
        gate = self.gate_conv(combined) # è¨ˆç®—æ¬Šé‡
        fused = (1 - gate) * proj_img + gate * proj_cli  # å¥—ç”¨æ¬Šé‡å¾—åˆ°embedding
        out = fused + self.final_fusion(fused) # å¥—ç”¨æ®˜å·®çµæ§‹ é€å…ƒç´ ç›¸åŠ 

        return out



class MyMultiModel(nn.Module):
    def __init__(self,
                 input_channels: int = 1,
                 num_classes: int = 2,
                 deep_supervision: bool = True,
                 clinical_csv_dir: str = '/home/admin/yuxin/data/Lab/model/UNet_base/nnunet_ins_data/data_test/nnUNet_raw/Dataset101',
                 freeze_bert: bool = True):
        super().__init__()
        self.input_channels = input_channels
        self.num_classes = num_classes
        self.deep_supervision = deep_supervision

        # ç”¨ DummyDecoder åªå­˜ deep_supervision å±¬æ€§ï¼Œé¿å…éè¿´
        class _DummyDecoder:
            def __init__(self, deep_supervision):
                self.deep_supervision = deep_supervision
        self.decoder = _DummyDecoder(self.deep_supervision)

        ## label encoder
        encoder = ClinicalDataLabelEncoder(clinical_csv_dir)

        # è®€å–è‡¨åºŠè³‡æ–™çš„é¡åˆ¥æ•¸
        self.num_location_classes = encoder.num_location_classes # 8
        self.num_t_stage_classes = encoder.num_t_stage_classes # 6
        self.num_n_stage_classes = encoder.num_n_stage_classes # 4
        self.num_m_stage_classes = encoder.num_m_stage_classes # 3

        # è®€å–è‡¨åºŠè³‡æ–™çš„ç¼ºå¤±idx
        self.missing_flag_location = encoder.missing_flag_location # 7
        self.missing_flag_t_stage = encoder.missing_flag_t_stage # 5
        self.missing_flag_n_stage = encoder.missing_flag_n_stage # 3
        self.missing_flag_m_stage = encoder.missing_flag_m_stage # 2




        # # ---------- æ–‡å­—ç·¨ç¢¼å™¨ ----------
        # # å‰µå»ºç¨ç«‹çš„æ–‡å­—ç·¨ç¢¼æ¨¡çµ„ï¼Œä¸æœƒè¢« torch.compile ç·¨è­¯
        # self.text_encoder = TextEncoderModule(freeze_bert)


        # ---------- å½±åƒç·¨ç¢¼å™¨ ----------
        self.encoder_stages = nn.ModuleList()
        
        # éšæ®µ 0: è¼¸å…¥é€šé“=1, è¼¸å‡ºé€šé“=32, æ­¥é•·=[1,1,1]
        stage0 = nn.Sequential(
            self._create_conv_block(input_channels, 32, kernel_size=3, stride=[1,1,1]),
            self._create_conv_block(32, 32, kernel_size=3, stride=[1,1,1])
        )
        self.encoder_stages.append(stage0)
        
        # éšæ®µ 1: è¼¸å…¥é€šé“=32, è¼¸å‡ºé€šé“=64, æ­¥é•·=[2,2,2]
        stage1 = nn.Sequential(
            self._create_conv_block(32, 64, kernel_size=3, stride=[2,2,2]),
            self._create_conv_block(64, 64, kernel_size=3, stride=[1,1,1])
        )
        self.encoder_stages.append(stage1)
        
        # éšæ®µ 2: è¼¸å…¥é€šé“=64, è¼¸å‡ºé€šé“=128, æ­¥é•·=[2,2,2]
        stage2 = nn.Sequential(
            self._create_conv_block(64, 128, kernel_size=3, stride=[2,2,2]),
            self._create_conv_block(128, 128, kernel_size=3, stride=[1,1,1])
        )
        self.encoder_stages.append(stage2)
        
        # éšæ®µ 3: è¼¸å…¥é€šé“=128, è¼¸å‡ºé€šé“=256, æ­¥é•·=[2,2,2]
        stage3 = nn.Sequential(
            self._create_conv_block(128, 256, kernel_size=3, stride=[2,2,2]),
            self._create_conv_block(256, 256, kernel_size=3, stride=[1,1,1])
        )
        self.encoder_stages.append(stage3)
        
        # éšæ®µ 4: è¼¸å…¥é€šé“=256, è¼¸å‡ºé€šé“=320, æ­¥é•·=[2,2,2]
        stage4 = nn.Sequential(
            self._create_conv_block(256, 320, kernel_size=3, stride=[2,2,2]),
            self._create_conv_block(320, 320, kernel_size=3, stride=[1,1,1])
        )
        self.encoder_stages.append(stage4)
        
        # éšæ®µ 5: è¼¸å…¥é€šé“=320, è¼¸å‡ºé€šé“=320, æ­¥é•·=[1,2,2]
        stage5 = nn.Sequential(
            self._create_conv_block(320, 320, kernel_size=3, stride=[1,2,2]),
            self._create_conv_block(320, 320, kernel_size=3, stride=[1,1,1])
        )
        self.encoder_stages.append(stage5)

        # ---------- è‡¨åºŠè³‡æ–™ç·¨ç¢¼å™¨ ----------
        # ä½¿ç”¨ nn.Embedding å°‡é›¢æ•£çš„è‡¨åºŠç‰¹å¾µè½‰æ›ç‚ºé€£çºŒå‘é‡
        # padding_idx ç¢ºä¿ç¼ºå¤±å€¼çš„åµŒå…¥å‘é‡ç‚ºé›¶
        self.emb_loc = nn.Embedding(self.num_location_classes, 8, padding_idx=self.missing_flag_location)
        self.emb_t   = nn.Embedding(self.num_t_stage_classes, 8, padding_idx=self.missing_flag_t_stage)
        self.emb_n   = nn.Embedding(self.num_n_stage_classes, 8, padding_idx=self.missing_flag_n_stage)
        self.emb_m   = nn.Embedding(self.num_m_stage_classes, 8, padding_idx=self.missing_flag_m_stage)

        # text_descriptions æŠ•å½±å±¤ - æ”¹é€²ç‰ˆ
        self.text_expand = nn.Sequential(
            nn.Linear(768, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(512, 320),
            nn.LayerNorm(320)
        )
        
        # è‡¨åºŠç‰¹å¾µæŠ•å½±å±¤ï¼Œå°‡æ‰€æœ‰embedding concate å¾Œ(dim=8*4)æŠ•å½±åˆ°èˆ‡å½±åƒç“¶é ¸å±¤(dim=320)ç›¸åŒçš„é€šé“æ•¸
        self.clinical_expand = nn.Sequential(
            nn.Linear(8 * 4, 256), # 8æ˜¯æ¯å€‹åµŒå…¥çš„ç¶­åº¦ï¼Œ4æ˜¯è‡¨åºŠç‰¹å¾µçš„æ•¸é‡
            nn.LeakyReLU(inplace=True),
            nn.Linear(256, 320) # è¼¸å‡ºç¶­åº¦èˆ‡å½±åƒç·¨ç¢¼å™¨ç“¶é ¸å±¤é€šé“æ•¸ä¸€è‡´
        )

        # ---------- ASPP æ¨¡å¡Šï¼ˆå¤šå±¤æ¬¡è¨­è¨ˆï¼‰----------
        # åœ¨ä¸åŒå±¤ä½¿ç”¨ä¸åŒå¼·åº¦çš„ ASPP
        # æ·±å±¤ï¼ˆä½è§£æåº¦ï¼‰ï¼šä½¿ç”¨å®Œæ•´ ASPP æ•ç²å¤§ç¯„åœä¸Šä¸‹æ–‡
        # æ·ºå±¤ï¼ˆé«˜è§£æåº¦ï¼‰ï¼šä¸ä½¿ç”¨ ASPPï¼Œä¿æŒç´°ç¯€
        self.aspp_modules = nn.ModuleList([
            None,  # stage0 (32é€šé“) - æœ€é«˜è§£æåº¦ï¼Œä¸ç”¨ ASPP
            None,  # stage1 (64é€šé“) - é«˜è§£æåº¦ï¼Œä¸ç”¨ ASPP  
            None,  # stage2 (128é€šé“) - ä¸­è§£æåº¦ï¼Œä¸ç”¨ ASPP
            ASPP3D(256, 256, rates=[1, 2]),      # stage3 (256é€šé“) - è¼•é‡ ASPP
            ASPP3D(320, 320, rates=[1, 2, 3]),   # stage4 (320é€šé“) - ä¸­åº¦ ASPP
            ASPP3D(320, 320, rates=[1, 2, 3, 4]) # stage5 (320é€šé“) - å®Œæ•´ ASPP (ç“¶é ¸å±¤)
        ])

        # ---------- é–€æ§æ··åˆ è·³èºé€£çµ(skip)èˆ‡è‡¨åºŠç‰¹å¾µ ----------
        self.multiscale_fusions = nn.ModuleList([
            GatedFusion(32),
            GatedFusion(64),
            GatedFusion(128),
            # None,
            # None,
            # None,
            GatedFusion(256),
            GatedFusion(320),
            GatedFusion(320)
        ])


        # # ---------- FiLMæ··åˆ,  Feature-wise Linear Modulation è·³èºé€£çµ(skip)èˆ‡è‡¨åºŠç‰¹å¾µ ----------
        # self.multiscale_fusions = nn.ModuleList([
        #     FiLMAddFusion(32,  320),
        #     FiLMAddFusion(64,  320),
        #     FiLMAddFusion(128, 320),
        #     FiLMAddFusion(256, 320),
        #     FiLMAddFusion(320, 320),
        #     FiLMAddFusion(320, 320)
        # ])

        # ---------- è§£ç¢¼å™¨ ----------
        self.transpconvs = nn.ModuleList() # ç”¨æ–¼ä¸Šæ¡æ¨£çš„è½‰ç½®å·ç©å±¤
        self.decoder_stages = nn.ModuleList() # è§£ç¢¼å™¨éšæ®µçš„å·ç©å±¤
        self.seg_layers = nn.ModuleList() # å½±åƒè³‡æ–™åˆ†å‰²é ­
        self.cli_layers = nn.ModuleList() # è‡¨åºŠè³‡æ–™åˆ†é¡é ­

        # è§£ç¢¼å™¨éšæ®µ 0 (å°æ‡‰ç·¨ç¢¼å™¨éšæ®µ5->4)
        # å¤šå°ºåº¦é–€æ§èåˆå½±åƒå’Œè‡¨åºŠç‰¹å¾µ
        # ä¸Šæ¡æ¨£å±¤
        self.transpconvs.append(nn.ConvTranspose3d(320, 320, kernel_size=[1,2,2], stride=[1,2,2]))
        # (ä¸Šæ¡æ¨£çµæœ concate è·³èºé€£æ¥) é™ç¶­
        self.decoder_stages.append(self._create_conv_block(320*3, 320, kernel_size=3, stride=1, num_convs=2))
        # ç”¨æ–¼æ·±åº¦ç›£ç£çš„åˆ†å‰²é ­
        self.seg_layers.append(nn.Conv3d(320, self.num_classes, kernel_size=1))
        # æ·±åº¦ç›£ç£çš„è‡¨åºŠè³‡æ–™é æ¸¬é ­
        self.cli_layers.append(self._create_cli_predictor(320))

        # è§£ç¢¼å™¨éšæ®µ 1 (å°æ‡‰ç·¨ç¢¼å™¨éšæ®µ4->3)
        self.transpconvs.append(nn.ConvTranspose3d(320, 256, kernel_size=2, stride=2))
        self.decoder_stages.append(self._create_conv_block(256*3, 256, kernel_size=3, stride=1, num_convs=2))
        self.seg_layers.append(nn.Conv3d(256, num_classes, kernel_size=1))
        self.cli_layers.append(self._create_cli_predictor(256))

        mul_coef = 3
        # è§£ç¢¼å™¨éšæ®µ 2 (å°æ‡‰ç·¨ç¢¼å™¨éšæ®µ3->2)
        self.transpconvs.append(nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2))
        self.decoder_stages.append(self._create_conv_block(128*mul_coef, 128, kernel_size=3, stride=1, num_convs=2))
        self.seg_layers.append(nn.Conv3d(128, num_classes, kernel_size=1))
        self.cli_layers.append(self._create_cli_predictor(128))

        # è§£ç¢¼å™¨éšæ®µ 3 (å°æ‡‰ç·¨ç¢¼å™¨éšæ®µ2->1)        
        self.transpconvs.append(nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2))
        self.decoder_stages.append(self._create_conv_block(64*mul_coef, 64, kernel_size=3, stride=1, num_convs=2))
        self.seg_layers.append(nn.Conv3d(64, num_classes, kernel_size=1))
        self.cli_layers.append(self._create_cli_predictor(64))

        # è§£ç¢¼å™¨éšæ®µ 4 (å°æ‡‰ç·¨ç¢¼å™¨éšæ®µ1->0)        
        self.transpconvs.append(nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2))
        self.decoder_stages.append(self._create_conv_block(32*mul_coef, 32, kernel_size=3, stride=1, num_convs=2))
        self.seg_layers.append(nn.Conv3d(32, num_classes, kernel_size=1))
        self.cli_layers.append(self._create_cli_predictor(32))

        # # åˆ†é¡é ­
        # self.loc_head = nn.Linear(128, self.missing_flag_location)  # è¼¸å‡ºæ‰€æœ‰é¡åˆ¥ï¼ˆä¸åŒ…æ‹¬Missingå°æ‡‰çš„ç´¢å¼•ï¼‰
        # self.t_head   = nn.Linear(128, self.missing_flag_t_stage)
        # self.n_head   = nn.Linear(128, self.missing_flag_n_stage)
        # self.m_head   = nn.Linear(128, self.missing_flag_m_stage)

        self.loc_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.LeakyReLU(0.01, inplace=True),
            nn.Dropout(0.2),
            nn.Linear(64, self.missing_flag_location)
        )
        self.t_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.LeakyReLU(0.01, inplace=True),
            nn.Dropout(0.2),
            nn.Linear(64, self.missing_flag_t_stage)
        )
        self.n_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.LeakyReLU(0.01, inplace=True),
            nn.Dropout(0.2),
            nn.Linear(64, self.missing_flag_n_stage)
        )
        self.m_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.LeakyReLU(0.01, inplace=True),
            nn.Dropout(0.2),
            nn.Linear(64, self.missing_flag_m_stage)
        )


        # åˆå§‹åŒ–æ¬Šé‡
        self.apply(self._init_weights)

    # ---------- å·¥å…· ----------
    def _create_conv_block(self, in_ch, out_ch, kernel_size, stride, num_convs=1):
        """è‡ªå‹•è¨ˆç®—æ¨¡å‹çš„å·ç©å±¤åƒæ•¸"""
        layers = []
        for i in range(num_convs):
            layers += [nn.Conv3d(in_ch if i == 0 else out_ch,
                                 out_ch, kernel_size, stride if i == 0 else 1,
                                 kernel_size // 2, bias=True),
                       InstanceNorm3d(out_ch),
                       LeakyReLU(0.01, True)]
        return nn.Sequential(*layers)

    def _create_cli_predictor(self, in_channels: int):
        """
        æ·±åº¦ç›£ç£ç”¨: å°æ¯å±¤ å‰µå»ºè‡¨åºŠå±¬æ€§é æ¸¬çš„æ¨¡å¡Š
        å›ºå®šè¼¸å‡º128ç¶­ äº¤çµ¦æ¯å€‹ç‰¹å¾µçš„åˆ†é¡é ­é€²è¡Œåˆ†é¡
        """
        return nn.Sequential(
            nn.AdaptiveAvgPool3d(1), # å…¨å±€å¹³å‡æ± åŒ–
            nn.Flatten(),
            nn.Linear(in_channels, 64), # å¯ä»¥æ˜¯æ›´å°çš„ MLP
            LeakyReLU(0.01, inplace=True),
            nn.Linear(64, 128) # è¼¸å‡ºåˆ°ä¸€å€‹å›ºå®šç¶­åº¦ï¼Œå†åˆ†ç™¼åˆ°å„å€‹è‡¨åºŠé ­
        )



    def _init_weights(self, m):
        if isinstance(m, (nn.Conv3d, nn.ConvTranspose3d, nn.Linear)):
            nn.init.kaiming_normal_(m.weight, 0.01)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, img, loc, t, n, m, text_descriptions):
        """
        Args: è¼¸å…¥æ¨¡å‹æ™‚ éœ€è¦å…¨éƒ¨ç‚º tensor
            img:          [B, C, D, H, W]
            loc:          [B]  æ•´æ•¸ç´¢å¼• (å« Missing ç´¢å¼•)
            t:            [B]  æ•´æ•¸ç´¢å¼• (å« Missing ç´¢å¼•)
            n:            [B]  æ•´æ•¸ç´¢å¼• (å« Missing ç´¢å¼•)
            m:            [B]  æ•´æ•¸ç´¢å¼• (å« Missing ç´¢å¼•)
            text_descriptions:     [B]  æ–‡å­—æè¿°åˆ—è¡¨
        Returns:
            seg_out:       [B, C, D, H, W] åˆ†å‰²é ­
            cli_out:       dict åŒ…å«è‡¨åºŠè³‡æ–™åˆ†é¡é ­è¼¸å‡º
                {
                    'location': [B, C=6]
                    't_stage':  [B, C=5]
                    'n_stage':  [B, C=3]
                    'm_stage':  [B, C=2]
                }
        """
        # ---------- å½±åƒç·¨ç¢¼ ----------
        skips = [] # ä¿å­˜æ¯å±¤ä¸‹æ¡æ¨£çš„embedding ç”¨æ–¼è·³èºé€£çµ


        x = img
        for i, stage in enumerate(self.encoder_stages):
            x = stage(x)
            skips.append(x) # ä¿å­˜æ¯å±¤ä¸‹æ¡æ¨£çš„embedding ç”¨æ–¼è·³èºé€£çµ

        bottleneck = skips[-1]

        # ---------- è‡¨åºŠ Embedding ----------
        loc_emb = self.emb_loc(loc) # [B] -> [B, C=8]
        t_emb   = self.emb_t(t)
        n_emb   = self.emb_n(n)
        m_emb   = self.emb_m(m)
        clinical_vec = torch.cat([loc_emb, t_emb, n_emb, m_emb], dim=1) # [B, C=8] -> [B, 8*4]
        clinical_feat = self.clinical_expand(clinical_vec) # [B, 8*4] -> [B, 320] (ç¬¦åˆç“¶é ¸å±¤ç¶­åº¦)

        # # ---------- æ–‡å­— Embedding ----------
        # # ä½¿ç”¨ç¨ç«‹çš„æ–‡å­—ç·¨ç¢¼æ¨¡çµ„ï¼Œä¸æœƒè¢« torch.compile ç·¨è­¯
        # text_embeddings = self.text_encoder(text_descriptions) # [B, 768]
        # text_embeddings = text_embeddings.to(img.device) # ç¢ºä¿èˆ‡æ¨¡å‹åœ¨åŒä¸€è¨­å‚™ä¸Š
        # clinical_feat = self.text_expand(text_embeddings) # [B, 768] -> [B, 320] (ç¬¦åˆç“¶é ¸å±¤ç¶­åº¦)


        # # ---------- èåˆ ----------
        # fused_skips = []
        # for i, skip in enumerate(skips):
        #     # # skips = [stage0, stage1, stage2, stage3, stage4, stage5]
        #     # # fused_skips[i] å°æ‡‰ encoder_stages[i] çš„è¼¸å‡º

        #     # # è·³éæ·ºå±¤ è¨ˆç®—é–€æ§æ··åˆçš„éç¨‹
        #     # # åªå°æ·±å±¤(ä½è§£æåº¦å±¤)é€²è¡Œèåˆï¼Œæ·ºå±¤ç›´æ¥ä½¿ç”¨åŸå§‹è·³èºé€£æ¥
        #     # if i < 3:  # stage0, stage1, stage2 (é«˜è§£æåº¦å±¤)
        #     #     fused_skips.append(None)  # ä½”ä½ï¼Œä¿æŒç´¢å¼•ä¸€è‡´
        #     #     continue
            
        #     fused_skip = self.multiscale_fusions[i](skip, clinical_feat)
        #     fused_skips.append(fused_skip)


        # ---------- ASPP + èåˆ ----------
        # é—œéµæ”¹é€²ï¼šå…ˆå°å½±åƒç‰¹å¾µé€²è¡Œ ASPP å¢å¼·ï¼Œå†èˆ‡è‡¨åºŠç‰¹å¾µèåˆ
        # é€™æ¨£ ASPP åªä½œç”¨æ–¼æœ‰ç©ºé–“èªç¾©çš„å½±åƒç‰¹å¾µ
        enhanced_skips = []
        for i, skip in enumerate(skips):
            # 1. å…ˆå°å½±åƒç‰¹å¾µé€²è¡Œ ASPPï¼ˆå¦‚æœè©²å±¤æœ‰é…ç½®ï¼‰
            if self.aspp_modules[i] is not None:
                skip_enhanced = self.aspp_modules[i](skip)
            else:
                skip_enhanced = skip  # æ·ºå±¤ä¸ä½¿ç”¨ ASPP
            
            enhanced_skips.append(skip_enhanced)
        
        # 2. å†å°‡å¢å¼·å¾Œçš„å½±åƒç‰¹å¾µèˆ‡è‡¨åºŠç‰¹å¾µèåˆ
        fused_skips = []
        for i, enhanced_skip in enumerate(enhanced_skips):
            fused_skip = self.multiscale_fusions[i](enhanced_skip, clinical_feat)
            fused_skips.append(fused_skip)

        # ---------- è§£ç¢¼å™¨ ----------
        lres = fused_skips[-1] # è§£ç¢¼å™¨è¼¸å…¥
        seg_out = [] # åˆ†å‰²é ­è¼¸å‡º å½±åƒ

        loc_out = [] # åˆ†é¡é ­è¼¸å‡º ä½ç½®
        t_out = []   # åˆ†é¡é ­è¼¸å‡º æ™‚é–“
        n_out = []   # åˆ†é¡é ­è¼¸å‡º æ•¸é‡
        m_out = []   # åˆ†é¡é ­è¼¸å‡º æ¨¡å¼

        # éæ­·æ‰€æœ‰è§£ç¢¼å™¨éšæ®µ
        for i in range(len(self.decoder_stages)):
            # 1. ä¸Šæ¡æ¨£
            lres = self.transpconvs[i](lres)

            ## 2. æ‹¼æ¥ embedding (æ³¨æ„æª¢æŸ¥ _create_conv_block çš„è¼¸å…¥ç¶­åº¦)

            # æ³¨æ„: è·³èºé€£æ¥ç´¢å¼•æ˜¯å¾å¾Œå¾€å‰å–
            # print(f"lres shape: {lres.shape}, skip shape: {skips[-(i+2)].shape}")  # èª¿è©¦è¼¸å‡º

            ## 2.1 åŸç‰ˆ ç›´æ¥æ‹šæ¥ 1.è·³èºé€£æ¥ 2.ä¸Šæ¡æ¨£çµæœ
            # skip = skips[-(i+2)]  # å¾-2é–‹å§‹ ä¸‹æ¬¡-3...ï¼Œè·³éç“¶é ¸å±¤
            # lres = torch.cat((lres, skip), dim=1)

            ## 2.2 äºŒç‰ˆ æ‹¼æ¥ 1.é–€æ§çµæœ 2.ä¸Šæ¡æ¨£çµæœ
            # skip_to_concat = fused_skips[-(i+2)]  # å¾-2é–‹å§‹ ä¸‹æ¬¡-3...ï¼Œè·³éç“¶é ¸å±¤
            # lres = torch.cat((lres, skip_to_concat), dim=1)

            # 2.3 ä¸‰ç‰ˆ æ‹¼æ¥ 1.é–€æ§çµæœ 2.è·³èºé€£æ¥ 3.ä¸Šæ¡æ¨£çµæœ
            skip_to_concat = fused_skips[-(i+2)]  # å¾-2é–‹å§‹ ä¸‹æ¬¡-3...ï¼Œè·³éç“¶é ¸å±¤
            skip = skips[-(i+2)]
            lres = torch.cat((lres, skip_to_concat, skip), dim=1)
            

            # # 2.4 æ ¹æ“šè§£æåº¦å±¤ç´šé¸æ“‡ä¸åŒçš„æ‹¼æ¥ç­–ç•¥
            # skip = skips[-(i+2)]  # å¾-2é–‹å§‹ ä¸‹æ¬¡-3...ï¼Œè·³éç“¶é ¸å±¤
            
            # if i < 2:  # å‰å…©å±¤è§£ç¢¼å™¨ï¼ˆä½è§£æåº¦å±¤ï¼‰
            #     skip_to_concat = fused_skips[-(i+2)]  # ä½¿ç”¨é–€æ§èåˆçµæœ
            #     lres = torch.cat((lres, skip_to_concat, skip), dim=1)  # ä¸‰è·¯æ‹¼æ¥
            # else:  # é«˜è§£æåº¦å±¤
            #     lres = torch.cat((lres, skip), dim=1)  # åŸå§‹nnUNeté¢¨æ ¼ï¼ŒäºŒè·¯æ‹¼æ¥



            # 3. ä½¿ç”¨conv å°æ‹¼æ¥å¾Œçš„embedding ä¸Šæ¡æ¨£
            lres = self.decoder_stages[i](lres)

            # 4. æ·±åº¦ç›£ç£è¼¸å‡º
            if self.deep_supervision:
                # åˆ†å‰²è¼¸å‡º
                seg_out.append(self.seg_layers[i](lres))
                # è‡¨åºŠè³‡æ–™è¼¸å‡º
                cli_raw_out = (self.cli_layers[i](lres))
                # è‡¨åºŠè³‡æ–™åˆ†é¡é ­
                loc_out.append(self.loc_head(cli_raw_out))
                t_out.append(self.t_head(cli_raw_out))
                n_out.append(self.n_head(cli_raw_out))
                m_out.append(self.m_head(cli_raw_out))

            # å¦‚æœé—œé–‰æ·±åº¦ç›£ç£ ä½†æ˜¯æœ€å¾Œä¸€å±¤è§£ç¢¼å™¨ é‚„æ˜¯è¦è¼¸å‡º
            elif i == (len(self.decoder_stages) - 1):
                # åˆ†å‰²è¼¸å‡º
                seg_out.append(self.seg_layers[-1](lres))
                # è‡¨åºŠè³‡æ–™è¼¸å‡º
                cli_raw_out = (self.cli_layers[-1](lres))
                # è‡¨åºŠè³‡æ–™åˆ†é¡é ­
                loc_out.append(self.loc_head(cli_raw_out))
                t_out.append(self.t_head(cli_raw_out))
                n_out.append(self.n_head(cli_raw_out))
                m_out.append(self.m_head(cli_raw_out))

        # åè½‰è¼¸å‡ºé †åº
        seg_out = seg_out[::-1] #[start, end, step]
        loc_out = loc_out[::-1]
        t_out = t_out[::-1]
        n_out = n_out[::-1]
        m_out = m_out[::-1]

        # ---------- å±¬æ€§é æ¸¬ ----------
        cli_out = {
            'location': loc_out, # loc_out[0] = [B, C=6]  (è‹¥å•Ÿå‹•æ·±åº¦ç›£ç£æœƒæœ‰5å€‹è§£æåº¦çš„è¼¸å‡º)
            't_stage':  t_out,   # t_out[0] = [B, C=5]
            'n_stage':  n_out,   # n_out[0] = [B, C=3]
            'm_stage':  m_out    # m_out[0] = [B, C=2]
        }

        # å¦‚æœé—œé–‰æ·±åº¦ç›£ç£ åªè¿”å›æœ€å¾Œä¸€å±¤çš„è¼¸å‡º (å·²ç¶“åœ¨å‰é¢è¨­å®š ä¸ç”¨åœ¨é€™è£¡åˆ¤æ–·æ˜¯å¦å•Ÿç”¨)
        return seg_out, cli_out
        

if __name__ == "__main__":
    # -------------------- è¶…åƒæ•¸ --------------------
    epochs = 10
    lr = 1e-4
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # -------------------- æ¨¡å‹ & å‡è³‡æ–™ --------------------
    model = MyMultiModel(input_channels=1, num_classes=2).to(device)
    img = torch.randn(2, 1, 64, 64, 64).to(device)          # B C D H W
    loc = torch.tensor([5, 2]).to(device)  # B 1ï¼Œbatch 1 è¼¸å…¥ 5ï¼Œbatch 2 è¼¸å…¥ 2
    t = torch.tensor([3, 1]).to(device)    # B 1ï¼Œbatch 1 è¼¸å…¥ 3ï¼Œbatch 2 è¼¸å…¥ 1
    n = torch.tensor([2, 0]).to(device)    # B 1ï¼Œbatch 1 è¼¸å…¥ 2ï¼Œbatch 2 è¼¸å…¥ 0
    m = torch.tensor([1, 0]).to(device)    # B 1ï¼Œbatch 1 è¼¸å…¥ 1ï¼Œbatch 2 è¼¸å…¥ 0
    
    # æ–‡å­—æè¿° (è¨“ç·´ç”¨å‡è³‡æ–™)
    text_descriptions_train = [
        "A computerized tomography scan reveals a colorectal cancer.",
        "A computerized tomography scan reveals a colorectal cancer."
    ]

    # å‡ GTï¼ˆåˆ†å‰²èˆ‡è‡¨åºŠæ¨™ç±¤éƒ½ç”¨ 0/1 éš¨ä¾¿å¡«ï¼‰
    seg_gt = torch.randint(0, 2, (2, 64, 64, 64)).long().to(device)  # èˆ‡æœ€çµ‚å±¤åŒç©ºé–“å°ºå¯¸
    loc_gt = torch.randint(0, model.missing_flag_location, (2,)).to(device)
    t_gt   = torch.randint(0, model.missing_flag_t_stage,   (2,)).to(device)
    n_gt   = torch.randint(0, model.missing_flag_n_stage,   (2,)).to(device)
    m_gt   = torch.randint(0, model.missing_flag_m_stage,   (2,)).to(device)

    # -------------------- loss & optimizer --------------------
    seg_criterion = nn.CrossEntropyLoss()
    cls_criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # -------------------- 10 epoch è¨“ç·´ --------------------
    model.train()
    for epoch in range(1, epochs+1):
        optimizer.zero_grad()

        seg_out, cli_out = model(img, loc, t, n, m, text_descriptions_train)  # å‰å‘å‚³æ’­

        # åˆ†å‰² lossï¼šå–æœ€å¾Œä¸€å±¤ï¼ˆé—œé–‰ deep_supervision æ™‚ï¼‰
        if isinstance(seg_out, list):
            seg_loss = seg_criterion(seg_out[0], seg_gt)
        else:
            seg_loss = seg_criterion(seg_out, seg_gt)

        # è‡¨åºŠ lossï¼šå–æœ€å¾Œä¸€å±¤é æ¸¬
        loc_loss = cls_criterion(cli_out['location'][-1], loc_gt)
        t_loss   = cls_criterion(cli_out['t_stage'][-1],   t_gt)
        n_loss   = cls_criterion(cli_out['n_stage'][-1],   n_gt)
        m_loss   = cls_criterion(cli_out['m_stage'][-1],   m_gt)

        loss = seg_loss + loc_loss + t_loss + n_loss + m_loss
        loss.backward()
        optimizer.step()

        print(f"Epoch {epoch:02d}/{epochs} | loss={loss.item():.4f}")

    # -------------------- æ¨è«–ç¤ºç¯„ --------------------
    with torch.no_grad():
        model.eval()
        seg_out, cli_out = model(img, loc, t, n, m, text_descriptions_train)  # å‰å‘å‚³æ’­
        print("\n=== eval shapes ===")
        for seg in (seg_out if isinstance(seg_out, list) else [seg_out]):
            print("seg:", seg.shape)
        for k, v in cli_out.items():
            print(k, [x.shape for x in v])

    print("Toy training done.")


    # ========== åƒæ•¸æª¢æŸ¥ ==========
    print("\n" + "="*50)
    print("æ¨¡å‹åƒæ•¸çµæ§‹åˆ†æ")
    print("="*50)
    
    all_params = list(model.parameters())
    trainable_params_list = [p for p in model.parameters() if p.requires_grad]
    trainable_params_set = set(trainable_params_list)  # ä¿®æ­£é€™è¡Œ

    total_params = sum(p.numel() for p in all_params)
    trainable_params_count = sum(p.numel() for p in trainable_params_list)
    frozen_params_count = total_params - trainable_params_count

    print(f"ç¸½åƒæ•¸é‡: {total_params / 1e6:.2f}M")
    print(f"å¯è¨“ç·´åƒæ•¸é‡: {trainable_params_count / 1e6:.2f}M")
    print(f"å‡çµåƒæ•¸é‡: {frozen_params_count / 1e6:.2f}M")
    
    if frozen_params_count > 0:
        print("\n=== å‡çµçš„åƒæ•¸å±¤ ===")
        for name, param in model.named_parameters():
            if not param.requires_grad:
                print(f"  â„ï¸  {name:50} | {str(param.shape):20} | {param.numel()/1e3:.1f}K")
    
    print("\n=== æ¨¡å‹çµæ§‹æ¦‚è¦½ (é¡ä¼¼ print(model)) ===")
    def print_module_structure(module, indent=0):
        prefix = "  " * indent
        if list(module.children()):
            print(f"{prefix}ğŸ“¦ {module.__class__.__name__}")
            for name, child in module.named_children():
                print(f"{prefix}  â”œâ”€ {name}: ", end="")
                if list(child.children()):
                    print()
                    print_module_structure(child, indent + 2)
                else:
                    param_count = sum(p.numel() for p in child.parameters())
                    trainable_count = sum(p.numel() for p in child.parameters() if p.requires_grad)
                    status = " âœ“" if trainable_count > 0 else " â„ï¸"
                    print(f"{child.__class__.__name__} ({param_count/1e3:.1f}K params){status}")
        else:
            param_count = sum(p.numel() for p in module.parameters())
            trainable_count = sum(p.numel() for p in module.parameters() if p.requires_grad)
            status = " âœ“" if trainable_count > 0 else " â„ï¸"
            print(f"{module.__class__.__name__} ({param_count/1e3:.1f}K params){status}")
    
    print_module_structure(model)

    # ========== å®Œæ•´è¼¸å…¥ç¯„ä¾‹ (åŒ…å«æ–‡å­—æè¿°) ==========
    print("\n" + "="*50)
    print("å®Œæ•´è¼¸å…¥ç¯„ä¾‹æ¸¬è©¦ (åŒ…å«æ–‡å­—æè¿°)")
    print("="*50)
    
    # é‡æ–°å»ºç«‹æ¨¡å‹å¯¦ä¾‹ï¼Œç¢ºä¿ä¹¾æ·¨ç‹€æ…‹
    model_test = MyMultiModel(input_channels=1, num_classes=2).to(device)
    
    # æº–å‚™å®Œæ•´è¼¸å…¥æ•¸æ“š
    batch_size = 2
    img_test = torch.randn(batch_size, 1, 64, 64, 64).to(device)  # [B, C, D, H, W]
    
    # è‡¨åºŠç‰¹å¾µ (ç¢ºä¿åœ¨æœ‰æ•ˆç¯„åœå…§)
    loc_test = torch.tensor([2, 5]).to(device)  # location indices
    t_test = torch.tensor([1, 3]).to(device)    # t_stage indices  
    n_test = torch.tensor([0, 2]).to(device)    # n_stage indices
    m_test = torch.tensor([0, 1]).to(device)    # m_stage indices
    
    # æ–‡å­—æè¿°ç¯„ä¾‹
    text_descriptions = [
        "A computerized tomography scan reveals a colorectal cancer located in the rectum region, with T stage T2, N stage N0, without distant metastasis.",
        "A computerized tomography scan reveals a colorectal cancer located in the sigmoid colon region, with T stage T4, N stage N2, with distant metastasis."
    ]
    
    print(f"è¼¸å…¥å½¢ç‹€æª¢æŸ¥:")
    print(f"  - å½±åƒ: {img_test.shape}")
    print(f"  - Location: {loc_test.shape} -> {loc_test.tolist()}")
    print(f"  - T_stage: {t_test.shape} -> {t_test.tolist()}")
    print(f"  - N_stage: {n_test.shape} -> {n_test.tolist()}")
    print(f"  - M_stage: {m_test.shape} -> {m_test.tolist()}")
    print(f"  - æ–‡å­—æè¿°æ•¸é‡: {len(text_descriptions)}")
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    model_test.eval()
    with torch.no_grad():
        try:
            seg_pred, cli_pred = model_test(img_test, loc_test, t_test, n_test, m_test, text_descriptions)
            
            print(f"\nâœ… æ¨¡å‹å‰å‘å‚³æ’­æˆåŠŸ!")
            print(f"åˆ†å‰²è¼¸å‡ºå½¢ç‹€:")
            if isinstance(seg_pred, list):
                for i, seg in enumerate(seg_pred):
                    print(f"  - å±¤ {i}: {seg.shape}")
            else:
                print(f"  - {seg_pred.shape}")
            
            print(f"è‡¨åºŠé æ¸¬è¼¸å‡ºå½¢ç‹€:")
            for feature, predictions in cli_pred.items():
                print(f"  - {feature}:")
                for i, pred in enumerate(predictions):
                    print(f"    å±¤ {i}: {pred.shape}")
            
            # é¡¯ç¤ºé æ¸¬çµæœ
            print(f"\né æ¸¬çµæœç¯„ä¾‹ (æœ€å¾Œä¸€å±¤):")
            print(f"  Location é æ¸¬æ©Ÿç‡åˆ†ä½ˆ: {torch.softmax(cli_pred['location'][-1], dim=1)}")
            print(f"  T_stage é æ¸¬æ©Ÿç‡åˆ†ä½ˆ: {torch.softmax(cli_pred['t_stage'][-1], dim=1)}")
            print(f"  N_stage é æ¸¬æ©Ÿç‡åˆ†ä½ˆ: {torch.softmax(cli_pred['n_stage'][-1], dim=1)}")
            print(f"  M_stage é æ¸¬æ©Ÿç‡åˆ†ä½ˆ: {torch.softmax(cli_pred['m_stage'][-1], dim=1)}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹å‰å‘å‚³æ’­å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "="*50)
    print("æ¸¬è©¦å®Œæˆ!")
    print("="*50)